{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13425350,"sourceType":"datasetVersion","datasetId":8521142}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install khmer-nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.44.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, Audio\nraw_dataset = load_dataset(\"rinabuoy/khm-asr-open\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install librosa","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(raw_dataset[\"train\"].features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = raw_dataset[\"train\"]\ntest_dataset = raw_dataset[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperTokenizer\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"khmer\", task=\"transcribe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"khmer\", task=\"transcribe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.generation_config.language = \"khmer\"\nmodel.generation_config.task = \"transcribe\"\nmodel.generation_config.forced_decoder_ids = None\nmodel.config.use_cache = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(test_dataset))\nprint(len(train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom khmernltk import word_tokenize\n\ndef clean_text(dataset):\n    text = dataset[\"sentence\"]\n\n    if re.search(r\"[A-Za-z]\", text):\n        return {\"sentence\": None}\n    tokenized_text = word_tokenize(\n        text, \n        return_tokens=False, \n        separator=\" \"\n    )\n    return {\"sentence\": tokenized_text if text else None}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = test_dataset.map(clean_text)\ntrain_dataset = train_dataset.map(clean_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.cast_column('audio', Audio(decode=False))\ntest_dataset = test_dataset.cast_column('audio', Audio(decode=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.filter(lambda x: x[\"sentence\"] is not None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = test_dataset.filter(lambda x: x[\"sentence\"] is not None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_dataset[500][\"sentence\"])\nprint(train_dataset[1]['sentence'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\ntest_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset.features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=False))\ntest_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import io\nimport soundfile as sf\nimport numpy as np\n\ndef prepare_dataset(batch):\n    # Decode audio from bytes\n    audio_bytes = batch[\"audio\"][\"bytes\"]\n    audio_array, sampling_rate = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\")\n    \n    # Resample if needed (soundfile loads at original sample rate)\n    if sampling_rate != 16000:\n        import librosa\n        audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=16000)\n        sampling_rate = 16000\n    \n    # Process the audio\n    batch[\"input_features\"] = feature_extractor(\n        audio_array, \n        sampling_rate=sampling_rate\n    ).input_features[0]\n    \n    batch[\"labels\"] = tokenizer(batch['sentence']).input_ids\n    \n    return batch\n\ntrain_dataset = train_dataset.map(prepare_dataset, remove_columns=[\"audio\", \"sentence\"])\ntest_dataset = test_dataset.map(prepare_dataset, remove_columns=[\"audio\", \"sentence\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nmetric = evaluate.load(\"wer\")\n# cer_metric = load_metric(\"cer\")\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # decode predictions and labels\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    # print the first 2 for inspection\n    print(\"Example predictions vs references:\")\n    for i in range(min(2, len(pred_str))):\n        print(f\"Prediction: {pred_str[i]}\")\n        print(f\"Reference : {label_str[i]}\")\n        print(\"---\")\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-kh-v2\",  # change to a repo name of your choice\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-4,\n    warmup_steps=80,\n    num_train_epochs=25,\n    gradient_checkpointing=True,\n    bf16=True,\n    eval_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=500,\n    eval_steps=500,\n    logging_steps=50,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor.save_pretrained(training_args.output_dir)\ntokenizer.save_pretrained(training_args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nfrom huggingface_hub import HfApi\nlogin(token=\"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\")\nrepo_name = \"dynann/whisper-small-khmer-v2\"\ntrainer.push_to_hub(repo_name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport soundfile as sf\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\n# Load the trained model\n# model_path = \"/kaggle/working/wav2vec2-xlsr-khmer-300m/checkpoint-4400\"\n    \n\n# model = Wav2Vec2ForCTC.from_pretrained(model_path)\nprocessor = Wav2Vec2Processor.from_pretrained(model)\n\nmodel.eval()\ndef transcribe_audio(audio_path):\n    \"\"\"Transcribe a single audio file\"\"\"\n    # Load audio\n    audio, sr = sf.read(audio_path)\n    if sr != 16000:\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n    \n    # Process audio\n    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    \n    with torch.no_grad():\n        inputs = {k: v for k, v in inputs.items()}\n        logits = model(**inputs).logits\n    \n    # Get predicted ids\n    predicted_ids = torch.argmax(logits, dim=-1)\n    \n    # Decode the ids to text\n    transcription = processor.decode(predicted_ids[0])\n    print(\"predicted_ids => \",predicted_ids[0])\n    return transcription\n\n# Test on a single file\nif __name__ == '__main__':\n    # Test on a sample from your dataset\n\n    test_audio_path = \"/kaggle/input/asr-large-km/data/wavs/00000.wav\"\n    print(\"🎤 Testing audio transcription...\")\n    print(f\"   Audio file: {test_audio_path}\")\n    transcription = transcribe_audio(test_audio_path)\n    print(f\"\\n📝 Predicted transcription => \", transcription)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze requirement.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}